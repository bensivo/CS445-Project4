{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNW8woVoN99YHxhuUPOO8C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bensivo/CS445-Project4/blob/main/style-transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras tensorflow\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "qpSbyieq4RqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The paper, uses the VGG model as a feature encoder.\n",
        "# Because VGG was trained for object recognition, its internal layers are a good representation of an image's shape / content.\n",
        "#\n",
        "# Source: https://franky07724-57962.medium.com/using-keras-pre-trained-models-for-feature-extraction-in-image-clustering-a142c6cdf5b1\n",
        "\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "\n",
        "model = VGG19(weights='imagenet', include_top=False)  # Setting include_top=False removes the final prediction layers, running this model as an encoder, not a classifier.\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "# Paper authors say they replaced \"MaxPooling\" with \"AveragePooling\" to get better results. This might be a future improvement.\n"
      ],
      "metadata": {
        "id": "WIi4298y4fEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define a new model, which extracts features from the VGG model\n",
        "# The \"outputs\" array here defines which layers to pull from. You can see all the layers in the summary above.\n",
        "feature_extraction_model = Model(\n",
        "    inputs=model.input,\n",
        "    outputs=[\n",
        "        model.get_layer('block4_conv1').output\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "C-lm1uk54iAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Original image\n",
        "original_img = Image.open('./cup.jpg')\n",
        "\n",
        "# Features of original image. Reshaping input data, then running it through our model.\n",
        "original_img_input = np.array(original_img)\n",
        "original_img_input = np.expand_dims(original_img_input, axis=0)\n",
        "original_img_input = preprocess_input(original_img_input)\n",
        "original_img_features = feature_extraction_model.predict(original_img_input)"
      ],
      "metadata": {
        "id": "zNyoyPKg4k8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image recreation from features taken from one or more layers of the original network\n",
        "#\n",
        "# The general process for this is:\n",
        "#   1. Generate an image of all random values\n",
        "#   2. N times:\n",
        "#       a. Run our image through the feature-extractor\n",
        "#       b. Calculate the loss between the original image's features and the new image's features\n",
        "#       c. Use tf's GradientTape to calculate the gradients of the loss\n",
        "#       d. Use tf's optimizer to apply the gradients to the image, nudging it closer to the original image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Random generated image input\n",
        "generated_img_input_tensor = tf.Variable(tf.random.uniform(original_img_input.shape, 0, 255))\n",
        "\n",
        "optimizer = tf.optimizers.Adam(learning_rate=0.1)\n",
        "for i in tqdm(range(1000)):\n",
        "    with tf.GradientTape() as tape:\n",
        "        generated_img_features = feature_extraction_model(generated_img_input_tensor)  # Notice we're calling feature_extraction_model as a function, not with .predict(). For some reason this is important\n",
        "        loss = tf.reduce_mean(tf.square(original_img_features - generated_img_features))\n",
        "\n",
        "    gradients = tape.gradient(loss, generated_img_input_tensor)\n",
        "    optimizer.apply_gradients([(gradients, generated_img_input_tensor)])\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(f'Loss @ iteration {i} = {loss}')\n",
        "        output = tf.clip_by_value(generated_img_input_tensor, 0, 255)\n",
        "        output = output.numpy().squeeze().astype(np.uint8)\n",
        "        tf.keras.preprocessing.image.save_img(f'generated_image_{i}.jpg', output)\n",
        "\n",
        "# Clip generated image to valid pixel range\n",
        "generated_image = tf.clip_by_value(generated_img_input_tensor, 0, 255)\n",
        "generated_image = generated_image.numpy().squeeze().astype(np.uint8)\n",
        "\n",
        "\n",
        "tf.keras.preprocessing.image.save_img('generated_image.jpg', generated_image)"
      ],
      "metadata": {
        "id": "Ud7D0t4v4nE2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}